Here we pass one batch of images, <code>x</code>, and pass it through the <code>Conv2d</code> layer and then through <code>ReLU</code>, which is the rectifier linear units activation function. ReLU's purpose is to introduce some non-linearity. 

<figure>
  <p><img src="img/relu.png" alt="ReLU activation function" width="200" height="200" class="center">
  <figcaption>ReLU activation function. https://sefiks.com/2017/08/21/relu-as-neural-networks-activation-function</figcaption>
</figure>


<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">





<div class="output_text output_subarea output_execute_result">
<pre>Output: tensor(2)</pre>
</div>

</div>

</div>
</div>

</div>
    </div>
  </div>

<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
<span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">





<div class="output_text output_subarea output_execute_result">
<pre>Output: tensor(0)</pre>
</div>

</div>

</div>
</div>

</div>
    </div>
  </div>


<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">





<div class="output_text output_subarea output_execute_result">
<pre>Output: tensor(0)</pre>
</div>

</div>

</div>
</div>

</div>
    </div>
  </div>
